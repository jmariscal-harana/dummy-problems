{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6X_5ynrG3KL8"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_durations(episode_durations, show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if not show_result:\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "    else:\n",
        "        display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iOn6Ehfo7mkC"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"Stores the transitions that the agent observes, allowing us to reuse this data later.\n",
        "\n",
        "    By sampling from it randomly, the transitions that build up a batch are decorrelated.\n",
        "    It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "        self.transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(self.transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "h1wh8NTV8Qah"
      },
      "outputs": [],
      "source": [
        "class DQN(torch.nn.Module):\n",
        "    def __init__(self, n_states, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(n_states, 128)\n",
        "        self.layer2 = torch.nn.Linear(128, 128)\n",
        "        self.layer3 = torch.nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.layer1(x))\n",
        "        x = torch.nn.functional.relu(self.layer2(x))\n",
        "\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NvsLZxSc6zdk"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"The main idea behind Q-learning is that, if we had a function Q*(s, a) that could tell us what our return R would be\n",
        "    \n",
        "    if we were to take an action in a given state, then we could easily construct a policy Ï€ that maximizes our rewards.\n",
        "    Since we don't know everything about the world, we don't have access to Q*.\n",
        "    But, since neural networks are universal function approximators, we can simply create one and train it to resemble Q*.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.env = gym.make(\"CartPole-v1\")\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "\n",
        "        self.episodes = 600\n",
        "\n",
        "        self.batch_size = 128  # number of transitions sampled randomly from the replay buffer\n",
        "        self.gamma = 0.99  # discount factor\n",
        "        self.epsilon_start = 0.9  # starting value of epsilon (probability of choosing random action)\n",
        "        self.epsilon_end = 0.05  # final value of epsilon\n",
        "        self.epsilon_decay = 1000  # rate of exponential decay of epsilon (higher -> slower decay)\n",
        "        self.tau = 0.005  # update rate of the target network\n",
        "        self.lr = 1e-4  # learning rate of the optimizer\n",
        "\n",
        "        self.policy_net = DQN(self.state_size, self.action_size).to(self.device)\n",
        "        self.target_net = DQN(self.state_size, self.action_size).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=self.lr, amsgrad=True)\n",
        "        self.memory = ReplayMemory(10000)\n",
        "\n",
        "        self.episode_durations = []\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def act(self, state):\n",
        "        sample = random.random()\n",
        "        eps_threshold = self.epsilon_end  + (self.epsilon_start - self.epsilon_end) * \\\n",
        "            math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
        "        self.steps_done += 1\n",
        "        if sample > eps_threshold:\n",
        "            with torch.no_grad():\n",
        "                return self.policy_net(state).max(1).indices.unsqueeze(0) # max(1) returns largest value of each row\n",
        "        else:\n",
        "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.int8, device=self.device)\n",
        "\n",
        "    def optimise(self):\n",
        "        # Convert batch-array of Transitions to Transition of batch-arrays (https://stackoverflow.com/a/19343/3343043)\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        batch = self.memory.transition(*zip(*transitions))\n",
        "\n",
        "        # Compute a mask of non-final states and concatenate the batch elements\n",
        "        # (a final state would've been the one after which simulation ended)\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                            batch.next_state)), device=self.device, dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                    if s is not None])\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        # Compute Q(s_t, a_t) across the batch for all actions and \n",
        "        # select Q(s_t) based on the actions taken during the act() step.\n",
        "        Q_policy = self.policy_net(state_batch).gather(1, action_batch).squeeze()\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states.\n",
        "        # Target values of actions for non_final_next_states are computed based\n",
        "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
        "        # Returns the next state value or 0 in case the state was final.\n",
        "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
        "        with torch.no_grad():\n",
        "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
        "\n",
        "        # Compute the target Q values according to the Bellman equation\n",
        "        Q_target = reward_batch + (self.gamma * next_state_values)\n",
        "\n",
        "        # Compute Huber loss for the temporal difference error\n",
        "        # (i.e. the difference between the predicted and the target Q values)\n",
        "        criterion = torch.nn.SmoothL1Loss()\n",
        "        loss = criterion(Q_policy, Q_target)\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def soft_update(self):\n",
        "        for policy_param, target_param in zip(self.policy_net.parameters(), self.target_net.parameters()):\n",
        "            target_param.data.copy_(self.tau*policy_param.data + (1.0 - self.tau)*target_param.data)\n",
        "\n",
        "    def train(self):\n",
        "        for e in range(self.episodes):\n",
        "            # Initialize the environment and get its state\n",
        "            state = torch.tensor(self.env.reset()[0], dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "\n",
        "            for t in count():\n",
        "                action = self.act(state)\n",
        "                observation, reward, done, _, _ = self.env.step(action.item())\n",
        "                reward = torch.tensor(reward, device=self.device).unsqueeze(0)\n",
        "                if done:\n",
        "                    next_state = None\n",
        "                else:\n",
        "                    next_state = torch.tensor(observation, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "\n",
        "                # Store the transition in memory\n",
        "                self.memory.push(state, action, next_state, reward)               \n",
        "\n",
        "                if len(self.memory) >= self.batch_size:\n",
        "                    # Optimise the policy network\n",
        "                    self.optimise()\n",
        "\n",
        "                    # Soft update the target network\n",
        "                    self.soft_update()\n",
        "                \n",
        "                if done:\n",
        "                    # self.episode_durations.append(t + 1)\n",
        "                    # plot_durations(self.episode_durations)\n",
        "\n",
        "                    if t < 500:\n",
        "                        print(f\"Episode: {e}. Reached {t} steps\")\n",
        "                    else:\n",
        "                        print(f\"Episode: {e}. Reached 500 steps. Saving the model.\")\n",
        "                        self.save(\"model_500_steps.pth\")\n",
        "                    break\n",
        "                else:\n",
        "                    state = next_state\n",
        "                    \n",
        "    def test(self, episodes=10, t_max=None, display=None):\n",
        "        if display == \"gui\":\n",
        "            self.env = gym.make(\"CartPole-v1\", render_mode='human')\n",
        "        elif display == \"video\":\n",
        "            tmp_env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "            trigger = lambda t: t % 10 == 0\n",
        "            self.env = gym.wrappers.RecordVideo(env=tmp_env, video_folder=\".\", name_prefix=\"cartpole\", episode_trigger=trigger)\n",
        "\n",
        "        self.policy_net.eval()\n",
        "        for e in range(episodes):\n",
        "            state = torch.tensor(self.env.reset()[0], dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            total_reward = 0\n",
        "\n",
        "            for t in count():\n",
        "                with torch.no_grad():\n",
        "                    action = self.policy_net(state).max(1).indices.unsqueeze(0)\n",
        "                observation, reward, done, _, _ = self.env.step(action.item())\n",
        "                total_reward += reward\n",
        "\n",
        "                if t % 100 == 0:\n",
        "                    print(f\"Step: {t}, Reward: {total_reward}\")\n",
        "\n",
        "                if done or t > t_max:\n",
        "                    print(f\"Episode: {e+1}/{episodes}, Score: {total_reward}\")\n",
        "                    break\n",
        "                else:\n",
        "                    state = torch.tensor(observation, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "\n",
        "        self.env.close()\n",
        "                \n",
        "    def save(self, name):\n",
        "        torch.save(self.policy_net.state_dict(), name)\n",
        "\n",
        "    def load(self, name):\n",
        "        try:\n",
        "            state_dict = torch.load(name, map_location=self.device)\n",
        "            self.policy_net.load_state_dict(state_dict)\n",
        "            self.target_net.load_state_dict(state_dict)\n",
        "            print(f\"Model loaded successfully from {name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading the model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = Agent()\n",
        "agent.train()\n",
        "agent.save(\"model_final.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Complete')\n",
        "plot_durations(agent.episode_durations, show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "n7JAM95mI6p1",
        "outputId": "3c95167c-c793-4046-ce28-53fa238c7a74"
      },
      "outputs": [],
      "source": [
        "agent = Agent()\n",
        "agent.load(\"../weights/model_500_steps.pth\")\n",
        "agent.test(episodes=1, t_max=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
